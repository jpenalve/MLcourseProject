{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLcourseProject - Deep Learning Your Brain\n",
    "## Classification of Movement Execution and Imagination using EEG Signals\n",
    "This repository is for the Advanced Topics in ML project Spring Semester 2019.\n",
    "\n",
    "# Group members: \n",
    "Tim Fischer, Özhan Özen, Joaquin Penalver-Andres\n",
    "\n",
    "# Project Description:\n",
    "The project will focus on classification of movement imagination and movement tasks using EEG signals. \n",
    "\n",
    "It is proven that thinking about different movement that one plans to execute, generates a different neural fooprint. This footprint can be detected by means of an EEG recording device. The goal of this project is to classify different movements that subjects may think of or execute. \n",
    "\n",
    "# DataSet\n",
    "We will base our project on an existing dataset (link: https://www.physionet.org/physiobank/database/eegmmidb/#experimental-protocol ). \n",
    "\n",
    "The physionet dataset contains the following classes, with 109 subjects and 64 EGG Channels (Subjects 88, 92 and 100 have overlapping events. Please exclude these subjects).\n",
    "\n",
    "LABELS\t|\tDESCRIPTION\n",
    "------------------------------\n",
    "0 \t|\tBaseline, eyes open           \n",
    "1 \t|\tBaseline, eyes closed         \n",
    "2 \t|\tMotor execution: Left Hand           \n",
    "3 \t|\tMotor execution: Right Hand          \n",
    "4 \t|\tMotor imagery: Left Hand           \n",
    "5 \t|\tMotor imagery: Right Hand          \n",
    "6 \t|\tMotor execution: Both Hands          \n",
    "7 \t|\tMotor execution: Both Feet           \n",
    "8 \t|\tMotor Im: Both Hands          \n",
    "9 \t|\tMotor imagery: Both Feet           \n",
    "\n",
    "We have chosen to include 8 classes (baselines are excluded) for classification. \n",
    "\n",
    "# Tools Used\n",
    "\n",
    "- Pytorch (For NN training)\n",
    "- MNE python (For downloading/loading eeg data and creating epochs)\n",
    "- Tensorflow (for tensorboard visualization)\n",
    "- Many other basic python modules for data processing\n",
    "- The final test were done on a Ubuntu with two GPU\n",
    "\n",
    "\n",
    "### Tensorboard\n",
    "------------\n",
    "To run the TensorBoard, open a new terminal, go to the ProjectCode folder and run the command `$ tensorboard --logdir=./logs --port=6006`. Then, open http://localhost:6006/ on your web browser. If you have logs in your log dir, you will see nice graphs ;)\n",
    "\n",
    "(logdir should point to the log directory of your created logs)\n",
    "\n",
    "Logs can be created by calling write_logs_for_tensorboard() -> Feel free to modify, extend this function or add more tensorboard functions for further analysis of network performance.\n",
    "\n",
    "\n",
    "# Examples, literature or code that we were inspired:\n",
    "\n",
    "- https://martinos.org/mne/stable/auto_examples/decoding/plot_decoding_csp_eeg.html#sphx-glr-auto-examples-decoding-plot-decoding-csp-eeg-py\n",
    "\n",
    "# The preprocessing and Networks Trained:\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "- The epochs are taken to cover 2s on movement (trigger) offset with 160 samples per second. \n",
    "- Each epoch data are normalized to have zero mean and standard deviation of 1.\n",
    "- Augmentation of data with gaussian noise and inpainting-like data removal for regularization are tried (both in time and channel axis), however, we could not detect significant difference.\n",
    "- 20 subjects were included, with 8 classes (no baseline).\n",
    "- Cropping the time axis in small windows (of 10 samples) was suggested in literature (Zhang,2018), we have followed the suggestion. Each network is trained with and without this technique.\n",
    "- For 3D CNN, the 64 channels are mapped to their locations in the head as a 2D grid (11x10). In order to make this location information a rectangle, zeros are added where there is no electrode on the grid.\n",
    "- Eventually the data shape were (nEpochs,nChannels,nSamples), (nEpochs,nChannelsX,nChannelsY,nSamples), (nEpochs*nWindows,nChannels,nSamples) or (nEpochs*nWindows,nChannelsX,nChannelsY,nSamples) depending on whether the time axis is cropped or not and whether it was 2D or 3D CNN.\n",
    "\n",
    "### Networks Trained\n",
    "\n",
    "1) 3D CNN\n",
    "- Layer 1 -- (32x1x11x10x10--cropped) or (32x1x11x10x320--non_cropped) + ExpoRU (CELU) + BatchNorm\n",
    "- Layer 2 -- (64x1x11x10x10--cropped) or (64x1x11x10x320--non_cropped) + ExpoRU (CELU) + BatchNorm\n",
    "- Layer 3 -- (128x1x11x10x10--cropped) or (128x1x11x10x320--non_cropped) + ExpoRU (CELU) + BatchNorm\n",
    "- Layer 4 -- (Flatten fully connected Linear, Droput=0.5) \n",
    "\n",
    "#### Note: All the kernel sizes were (3,3,3) with stride 1, and padding such that there were no reduction on the size.\n",
    "\n",
    "2) 2D CNN \n",
    "\n",
    "- Layer 1 -- (32x1x64x10--cropped) or (32x1x64x320--non_cropped) + ExpoRU (CELU) + BatchNorm\n",
    "- Layer 2 -- (64x1x64x10--cropped) or (64x1x64x320--non_cropped) + ExpoRU (CELU) + BatchNorm\n",
    "- Layer 3 -- (128x1x64x10--cropped) or (128x1x64x320--non_cropped) + ExpoRU (CELU) + BatchNorm\n",
    "- Layer 4 -- (Flatten fully connected Linear, Droput=0.5) \n",
    "\n",
    "#### Note: All the kernel sizes were (7,3) with stride 1, and padding such that there were no reduction on the size.\n",
    "\n",
    "### For all the trainings, ADAM optimizer (lr:1e-3, weight decay:1e-4, scheduler with 20 steps and gamma 0.5), and Cross Entropy Loss (with sigmoid activation in the output) are used. Batch size was either 128 or 256.\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test sets for both CNNs with both cropped and non-cropped (of time axis) epochs are below. The in-class accuracy on the right is given for 3D-CNN cropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Results/Figures/Tables.png\" alt=\"Drawing\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, cropping the time axis in time windows (of 10 samples) makes a huge difference\n",
    "\n",
    "The training progress for accuracy and loss for cropped epochs are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Results/Figures/cropped.png\" alt=\"Drawing\" style=\"width: 1200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training progress for accuracy and loss for non-cropped epochs are below. You could see the huge overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Results/Figures/notcropped.png\" alt=\"Drawing\" style=\"width: 1200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Code: How-to\n",
    "In order to run the project, you need to open the 'Main' jupyter notebook and run it from top to bottom.\n",
    "\n",
    "All parameters necessary for adapting the classification can be modified inside the config/<myconfig.py> files.\n",
    "\n",
    "To store specific settings, just add a class to myconfig.py, inheriting from DefaultConfig.\n",
    "\n",
    "Put your configs which shall be evaluated inside the list_of_configs in the myconfig.py module.\n",
    "\t\n",
    "Inside the main.py: Select your config via myList = myconfig.list_of_configs\n",
    "\n",
    "In case of supplementary optimizers or nn, please add them to the optimizers.py or neural_nets package. Adapt the optimizer_list or nn_list (+nn_models_getter.py) the  in the defaultconifg.py respectively.\n",
    "\n",
    "## Example config file to run any NN implemented/tested\n",
    "\n",
    "\n",
    "#### Config class name to include in the list at the bottom.\n",
    "class Config3DCNN_NOTCropped(DefaultConfig):\n",
    "    verbose = 'CRITICAL'\n",
    "    \n",
    "    config_name = '3D CNN'\n",
    "    config_remark = '3D CNN'\n",
    "   \n",
    "   #### Number of subjects.\n",
    "    nSubj = 20 \n",
    "    selected_subjects = selected_subjects[:nSubj]\n",
    "    \n",
    "   #### Selection of the network and optimizer, these names have to be in .py files.\n",
    "    nn_list = ['ConvNet3D']  # Extend if you want more. Add them in the nn_models_getter.py module\n",
    "    nn_selection_idx = 0\n",
    "    optimizer_list = ['Adam']  # Extend if you want more. Add them in the optimizers.py module\n",
    "    optimizer_selection_idx = 0  # Idx corresponds to entry optimizer_list (find below)\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "   #### Setting up a scheduler for learning rate.\n",
    "    scheduler = True  \n",
    "    schStepSize = 20\n",
    "    schGamma = 0.5\n",
    "    \n",
    "   #### Number of epochs and early stopping settings.\n",
    "    num_of_epochs = 50\n",
    "    batch_size = 128\n",
    "    use_early_stopping = True\n",
    "    es_patience = num_of_epochs\n",
    "    \n",
    "   #### Normalization/augmentation settings\n",
    "    normalize = True\n",
    "    augment_with_gauss_noise = False\n",
    "    augmentation_factor = 2\n",
    "    augment_std_gauss = 0.2\n",
    "    dropOut = False\n",
    "    dropOutChOnly = False\n",
    "    dropOutTimeOnly = False\n",
    "    dropOutTilePerc = 0.5\n",
    "    dropOutTimeTile = 40\n",
    "    dropOutChannelTile = 8\n",
    "    \n",
    "   #### Epoch Settings\n",
    "    time_before_event_s = 0.0  # Epochsize parameter: Start time before event.\n",
    "    time_after_event_s = 2.0  # Epochsize parameter: Time after event.\n",
    "    downSample = 1\n",
    "    \n",
    "   #### To make number of data points dividable with 10\n",
    "    show_events_distribution = False\n",
    "    removeLastData = True\n",
    "    \n",
    "   #### To make channel dimension 2d, or cropping the time axis.\n",
    "    Elec2D = True\n",
    "    wSize = 10\n",
    "    wCropped = False\n",
    "    \n",
    "    \n",
    "#### All the classes in this list will be trained.\n",
    "list_of_configs = [Config3DCNN_Cropped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START OF THE CODE BELOW\n",
    "# START OF THE CODE BELOW\n",
    "# START OF THE CODE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from visualisations import eeg_sample_plot, events_distribution_plot, curve_name_gen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import ChannelsVoltageDataset\n",
    "from neural_nets.nn_models_getter import get_nn_model\n",
    "from optimizers import get_optimizer\n",
    "from utils_train import fit, test, plot_all_metrics, plot_val_metrics, final_test_acc\n",
    "from configs import configs_ozhan, configs_tim, configs_joaquin, configs_final\n",
    "from data_loader_creation import get_dataloader_objects\n",
    "from classification_results import results_storer\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the configuration (configs_final is the final one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" USER: SELECT THE CONFIGURATION YOU NEED \"\"\"\n",
    "myList = configs_final.list_of_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainining, plotting and saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curves = {}\n",
    "\n",
    "for my_cfg in myList:\n",
    "    \n",
    "    curve_name_gen(my_cfg)\n",
    "    \n",
    "    \"\"\" PREPARE DATALOADERS \"\"\"\n",
    "    train_dl, val_dl, test_dl, input_dimension_, output_dimension_ = get_dataloader_objects(my_cfg)\n",
    "\n",
    "    \n",
    "    \"\"\"CLASSIFICATION\"\"\"\n",
    "    # Get the model\n",
    "    model_untrained = get_nn_model(my_cfg, input_dimension=input_dimension_,output_dimension=output_dimension_)\n",
    "\n",
    "    # Get the optimizer\n",
    "    optimizer = get_optimizer(my_cfg, model_untrained.parameters())\n",
    "\n",
    "    # Train and show validation loss\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies, model_trained, time_spent_for_training_s =\\\n",
    "        fit(train_dl, val_dl, model_untrained, optimizer, my_cfg)\n",
    "    \n",
    "    curves = train_losses, train_accuracies, val_losses, val_accuracies, model_trained, time_spent_for_training_s\n",
    "    training_curves[my_cfg.curve_name] = curves\n",
    "    \n",
    "    # Report final accuracy\n",
    "    test_loss, test_accuracy =  final_test_acc(model_trained, test_dl, my_cfg)\n",
    "    \n",
    "    # Save the model\n",
    "    todays_date = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "    todays_date = 'TrainedModel/'+todays_date\n",
    "    todays_date_all = todays_date + '_all'\n",
    "    torch.save(model_trained.state_dict(), todays_date)\n",
    "    torch.save(model_trained, todays_date_all)\n",
    "\n",
    "    # Store the results\n",
    "    results_storer.store_results_for_plot(my_cfg,test_loss, test_accuracy, train_losses,\n",
    "                                 train_accuracies, time_spent_for_training_s, val_losses, val_accuracies)\n",
    "\n",
    "    results_storer.store_results(my_cfg, model_trained, optimizer, test_loss, test_accuracy, train_losses,\n",
    "                                 train_accuracies, time_spent_for_training_s, val_losses, val_accuracies, test_dl)\n",
    "\n",
    "\"\"\"Creates a figure with results and saves it\"\"\"\n",
    "FigName = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "FigName = \"Results/Figures/\"+ FigName+\".eps\"\n",
    "plot_all_metrics(training_curves)\n",
    "plt.savefig(FigName, format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
